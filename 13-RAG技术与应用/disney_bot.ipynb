{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d069084",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "迪士尼客服 RAG 助手 - 完整 Python 代码示例\n",
    " \n",
    "本代码实现了《RAG（Retrieval Augmented Generation）技术与应用》案例中描述的全流程。\n",
    "它将演示如何处理多种格式的文档（Word），提取文本、表格和图片，\n",
    "然后使用 Embedding 模型（text-embedding-v4, CLIP）和 FAISS 向量库构建一个\n",
    "能够回答文本和图片相关问题的智能问答系统。\n",
    " \n",
    "在运行前，请确保完成以下准备工作：\n",
    " \n",
    "1. 安装所有必需的 Python 库:\n",
    "   pip install openai \"faiss-cpu\" python-docx PyMuPDF Pillow pytesseract transformers torch requests\n",
    " \n",
    "2. 安装 Google Tesseract OCR 引擎:\n",
    "   - Windows: 从 https://github.com/UB-Mannheim/tesseract/wiki 下载并安装。\n",
    "   - macOS: brew install tesseract\n",
    "   - Linux (Ubuntu): sudo apt-get install tesseract-ocr\n",
    "   请确保 tesseract 的可执行文件路径已添加到系统的 PATH 环境变量中。\n",
    " \n",
    "3. 设置环境变量:\n",
    "   - DASHSCOPE_API_KEY: 您从阿里云百炼平台获取的 API Key。\n",
    "   - HF_TOKEN: (可选) 您的 Hugging Face Token，用于下载 CLIP 模型，避免手动确认。\n",
    " \n",
    "\"\"\"\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import faiss\n",
    "from openai import OpenAI\n",
    "from docx import Document as DocxDocument\n",
    "import fitz  # PyMuPDF\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "# Step0. 全局配置与模型加载\n",
    " \n",
    "# 检查环境变量\n",
    "DASHSCOPE_API_KEY = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "if not DASHSCOPE_API_KEY:\n",
    "    raise ValueError(\"错误：请设置 'DASHSCOPE_API_KEY' 环境变量。\")\n",
    " \n",
    "# 初始化百炼兼容的 OpenAI 客户端\n",
    "client = OpenAI(\n",
    "    api_key=DASHSCOPE_API_KEY,\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffeb41b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCS_DIR = \"disney_knowledge_base\"\n",
    "#IMG_DIR = os.path.join(DOCS_DIR, \"images\")\n",
    "TEXT_EMBEDDING_MODEL = \"text-embedding-v4\"\n",
    "TEXT_EMBEDDING_DIM = 1024\n",
    "#IMAGE_EMBEDDING_DIM = 512 # CLIP 'vit-base-patch32' 模型的输出维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "242ac144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1. 文档解析与内容提取\n",
    "def parse_docx(file_path):\n",
    "    \"\"\"解析 DOCX 文件，提取文本和表格（转为Markdown）。\"\"\"\n",
    "    doc = DocxDocument(file_path)\n",
    "    content_chunks = []\n",
    "     \n",
    "    for element in doc.element.body:\n",
    "        if element.tag.endswith('p'):\n",
    "            # 处理段落\n",
    "            paragraph_text = \"\"\n",
    "            for run in element.findall('.//w:t', {'w': 'http://schemas.openxmlformats.org/wordprocessingml/2006/main'}):\n",
    "                paragraph_text += run.text if run.text else \"\"\n",
    "             \n",
    "            if paragraph_text.strip():\n",
    "                content_chunks.append({\"type\": \"text\", \"content\": paragraph_text.strip()})\n",
    "                 \n",
    "        elif element.tag.endswith('tbl'):\n",
    "            # 处理表格\n",
    "            md_table = []\n",
    "            table = [t for t in doc.tables if t._element is element][0]\n",
    "             \n",
    "            if table.rows:\n",
    "                # 添加表头\n",
    "                header = [cell.text.strip() for cell in table.rows[0].cells]\n",
    "                md_table.append(\"| \" + \" | \".join(header) + \" |\")\n",
    "                md_table.append(\"|\" + \"---|\"*len(header))\n",
    "                 \n",
    "                # 添加数据行\n",
    "                for row in table.rows[1:]:\n",
    "                    row_data = [cell.text.strip() for cell in row.cells]\n",
    "                    md_table.append(\"| \" + \" | \".join(row_data) + \" |\")\n",
    "                 \n",
    "                table_content = \"\\n\".join(md_table)\n",
    "                if table_content.strip():\n",
    "                    content_chunks.append({\"type\": \"table\", \"content\": table_content})\n",
    "     \n",
    "    return content_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "502947b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step2. Embedding 与索引构建\n",
    "def get_text_embedding(text):\n",
    "    \"\"\"获取文本的 Embedding。\"\"\"\n",
    "    response = client.embeddings.create(\n",
    "        model=TEXT_EMBEDDING_MODEL,\n",
    "        input=text,\n",
    "        dimensions=TEXT_EMBEDDING_DIM\n",
    "    )\n",
    "    return response.data[0].embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be293199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_knowledge_base(docs_dir, img_dir):\n",
    "    \"\"\"构建完整的知识库，包括解析、切片、Embedding和索引。\"\"\"\n",
    "    print(\"\\n--- 步骤 1 & 2: 正在解析、Embedding并索引知识库 ---\")\n",
    "     \n",
    "    metadata_store = []\n",
    "    text_vectors = []\n",
    "    image_vectors = []\n",
    "     \n",
    "    doc_id_counter = 0\n",
    " \n",
    "    # 处理Word文档\n",
    "    for filename in os.listdir(docs_dir):\n",
    "        if filename.startswith('.') or os.path.isdir(os.path.join(docs_dir, filename)):\n",
    "            continue\n",
    "             \n",
    "        file_path = os.path.join(docs_dir, filename)\n",
    "        if filename.endswith(\".docx\"):\n",
    "            print(f\"  - 正在处理: {filename}\")\n",
    "            chunks = parse_docx(file_path)\n",
    "             \n",
    "            for chunk in chunks:\n",
    "                metadata = {\n",
    "                    \"id\": doc_id_counter,\n",
    "                    \"source\": filename,\n",
    "                    \"page\": 1\n",
    "                }\n",
    "                 \n",
    "                if chunk[\"type\"] == \"text\" or chunk[\"type\"] == \"table\":\n",
    "                    text = chunk[\"content\"]\n",
    "                    if not text.strip(): \n",
    "                        continue\n",
    "                     \n",
    "                    metadata[\"type\"] = \"text\"\n",
    "                    metadata[\"content\"] = text\n",
    "                     \n",
    "                    vector = get_text_embedding(text)\n",
    "                    text_vectors.append(vector)\n",
    "                    metadata_store.append(metadata)\n",
    "                    doc_id_counter += 1\n",
    " \n",
    "    # 处理images目录中的独立图片文件\n",
    "    # print(\"  - 正在处理独立图片文件...\")\n",
    "    # for img_filename in os.listdir(img_dir):\n",
    "    #     if img_filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "    #         img_path = os.path.join(img_dir, img_filename)\n",
    "    #         print(f\"    - 处理图片: {img_filename}\")\n",
    "             \n",
    "    #         img_text_info = image_to_text(img_path)\n",
    "             \n",
    "    #         metadata = {\n",
    "    #             \"id\": doc_id_counter,\n",
    "    #             \"source\": f\"独立图片: {img_filename}\",\n",
    "    #             \"type\": \"image\",\n",
    "    #             \"path\": img_path,\n",
    "    #             \"ocr\": img_text_info[\"ocr\"],\n",
    "    #             \"page\": 1\n",
    "    #         }\n",
    "             \n",
    "    #         vector = get_image_embedding(img_path)\n",
    "    #         image_vectors.append(vector)\n",
    "    #         metadata_store.append(metadata)\n",
    "    #         doc_id_counter += 1\n",
    " \n",
    "    # 创建 FAISS 索引\n",
    "    # 文本索引\n",
    "    text_index = faiss.IndexFlatL2(TEXT_EMBEDDING_DIM)\n",
    "    text_index_map = faiss.IndexIDMap(text_index)\n",
    "    text_ids = [m[\"id\"] for m in metadata_store if m[\"type\"] == \"text\"]\n",
    "    if text_vectors:  # 只有当有文本向量时才添加到索引\n",
    "        text_index_map.add_with_ids(np.array(text_vectors).astype('float32'), np.array(text_ids))\n",
    "     \n",
    "    # 图像索引\n",
    "    # image_index = faiss.IndexFlatL2(IMAGE_EMBEDDING_DIM)\n",
    "    # image_index_map = faiss.IndexIDMap(image_index)\n",
    "    # image_ids = [m[\"id\"] for m in metadata_store if m[\"type\"] == \"image\"]\n",
    "    # if image_vectors:  # 只有当有图像向量时才添加到索引\n",
    "    #     image_index_map.add_with_ids(np.array(image_vectors).astype('float32'), np.array(image_ids))\n",
    "     \n",
    "    print(f\"索引构建完成。共索引 {len(text_vectors)} 个文本片段\")\n",
    "     \n",
    "    return metadata_store, text_index_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59f08f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step3. RAG 问答流程\n",
    "def rag_ask(query, metadata_store, text_index, k=3):\n",
    "    \"\"\"\n",
    "    执行完整的 RAG 流程：检索 -> 构建Prompt -> 生成答案\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- 收到用户提问: '{query}' ---\")\n",
    "     \n",
    "    # 步骤 1: 检索\n",
    "    print(\"  - 步骤 1: 向量化查询并进行检索...\")\n",
    "    retrieved_context = []\n",
    "     \n",
    "    # 文本检索\n",
    "    query_text_vec = np.array([get_text_embedding(query)]).astype('float32')\n",
    "    distances, text_ids = text_index.search(query_text_vec, k)\n",
    "    for i, doc_id in enumerate(text_ids[0]):\n",
    "        if doc_id != -1:\n",
    "            # 通过ID在元数据中查找\n",
    "            match = next((item for item in metadata_store if item[\"id\"] == doc_id), None)\n",
    "            if match:\n",
    "                retrieved_context.append(match)\n",
    "                print(f\"    - 文本检索命中 (ID: {doc_id}, 距离: {distances[0][i]:.4f})\")\n",
    " \n",
    "    # 图像检索 (使用CLIP文本编码器)\n",
    "    # 简单判断是否需要检索图片\n",
    "    # if any(keyword in query.lower() for keyword in [\"海报\", \"图片\", \"长什么样\", \"看看\", \"万圣节\", \"聚在一起\"]):\n",
    "    #     print(\"  - 检测到图像查询关键词，执行图像检索...\")\n",
    "    #     query_clip_vec = np.array([get_clip_text_embedding(query)]).astype('float32')\n",
    "    #     distances, image_ids = image_index.search(query_clip_vec, 1) # 只找最相关的1张图\n",
    "    #     for i, doc_id in enumerate(image_ids[0]):\n",
    "    #         if doc_id != -1:\n",
    "    #             match = next((item for item in metadata_store if item[\"id\"] == doc_id), None)\n",
    "    #             if match:\n",
    "    #                 # 将OCR内容也加入上下文\n",
    "    #                 context_text = f\"找到一张相关图片，图片路径: {match['path']}。图片上的文字是: '{match['ocr']}'\"\n",
    "    #                 retrieved_context.append({\"type\": \"image_context\", \"content\": context_text, \"metadata\": match})\n",
    "    #                 print(f\"    - 图像检索命中 (ID: {doc_id}, 距离: {distances[0][i]:.4f})\")\n",
    "     \n",
    "    # 步骤 2: 构建 Prompt 并生成答案\n",
    "    print(\"  - 步骤 2: 构建 Prompt...\")\n",
    "    context_str = \"\"\n",
    "    for i, item in enumerate(retrieved_context):\n",
    "        content = item.get('content', '')\n",
    "        source = item.get('metadata', {}).get('source', item.get('source', '未知来源'))\n",
    "        context_str += f\"背景知识 {i+1} (来源: {source}):\\n{content}\\n\\n\"\n",
    "         \n",
    "    prompt = f\"\"\"你是一个迪士尼客服助手。请根据以下背景知识，用友好和专业的语气回答用户的问题。请只使用背景知识中的信息，不要自行发挥。\n",
    "[背景知识]\n",
    "{context_str}\n",
    "[用户问题]\n",
    "{query}\n",
    "\"\"\"\n",
    "    print(\"--- Prompt Start ---\")\n",
    "    print(prompt)\n",
    "    print(\"--- Prompt End ---\")\n",
    "     \n",
    "    print(\"\\n  - 步骤 3: 调用 LLM 生成最终答案...\")\n",
    "    try:\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"qwen-plus\", # 使用一个强大的模型进行生成\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"你是一个迪士尼客服助手。\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        final_answer = completion.choices[0].message.content\n",
    "         \n",
    "        # 答案后处理：如果上下文中包含图片，提示用户\n",
    "        # image_path_found = None\n",
    "        # for item in retrieved_context:\n",
    "        #     if item.get(\"type\") == \"image_context\":\n",
    "        #         image_path_found = item.get(\"metadata\", {}).get(\"path\")\n",
    "        #         break\n",
    "         \n",
    "        # if image_path_found:\n",
    "        #     final_answer += f\"\\n\\n(同时，我为您找到了相关图片，路径为: {image_path_found})\"\n",
    " \n",
    "    except Exception as e:\n",
    "        final_answer = f\"调用LLM时出错: {e}\"\n",
    " \n",
    "    print(\"\\n--- 最终答案 ---\")\n",
    "    print(final_answer)\n",
    "    return final_answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
