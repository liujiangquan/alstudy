# -*- coding: utf-8 -*-
"""
è¯­ä¹‰åˆ‡ç‰‡ç­–ç•¥
- åŸºäºå¥å­è¾¹ç•Œè¿›è¡Œåˆ‡åˆ†
- ä½¿ç”¨è¿æ¥è¯ä¸ä¸»é¢˜é‡å åº¦ä¿æŒè¯­ä¹‰å®Œæ•´æ€§
- æ”¯æŒå¥å­çº§é‡å ï¼Œé¿å…ä¸Šä¸‹æ–‡å‰²è£‚
"""

import re
from typing import List, Set


def split_into_sentences(text: str) -> List[str]:
    """æŒ‰ä¸­è‹±å¥å­ç»“æŸç¬¦åˆ‡åˆ†ï¼Œä¿ç•™ç»“æŸç¬¦ä¸ç´§éšçš„å³å¼•å·/æ‹¬å·ã€‚"""
    if not text:
        return []

    enders = set("ã€‚ï¼ï¼Ÿ!?ï¼›;")
    right_closers = set('â€â€™"\')ï¼‰ã€‘ã€ã€‹ã€‰]')
    sentences: List[str] = []
    buffer: List[str] = []

    i = 0
    n = len(text)
    while i < n:
        ch = text[i]
        buffer.append(ch)

        if ch in enders:
            j = i + 1
            while j < n and text[j] in right_closers:
                buffer.append(text[j])
                j += 1
            sentences.append("".join(buffer).strip())
            buffer = []
            i = j
            continue
        i += 1

    tail = "".join(buffer).strip()
    if tail:
        sentences.append(tail)

    # åˆå¹¶è¿‡çŸ­çš„å¥å­ï¼ˆå¦‚æ ‡é¢˜/æ–­è¡Œï¼‰
    merged: List[str] = []
    for s in sentences:
        if merged and len(s) < 8:
            merged[-1] = (merged[-1] + (" " if not merged[-1].endswith(("ã€‚", "ï¼", "ï¼Ÿ", ".", "!", "?")) else "") + s).strip()
        else:
            merged.append(s)
    return [s for s in merged if s]


CN_CONNECTIVES = [
    "ä½†æ˜¯", "ç„¶è€Œ", "å› æ­¤", "æ­¤å¤–", "å¦å¤–", "åŒæ—¶", "è€Œä¸”", "å¹¶ä¸”", "è€Œ", "æ‰€ä»¥", "å› ä¸º", "ä¾‹å¦‚", "æ¯”å¦‚",
    "æ€»ä¹‹", "é¦–å…ˆ", "å…¶æ¬¡", "æœ€å", "å¦ä¸€æ–¹é¢", "ç»¼ä¸Š", "åŸºäºæ­¤", "æ¢è¨€ä¹‹", "ä¹Ÿå°±æ˜¯è¯´", "å³", "å³ä½¿",
]
EN_CONNECTIVES = [
    "however", "therefore", "moreover", "in addition", "furthermore", "besides",
    "meanwhile", "thus", "hence", "so", "because", "for example", "for instance",
    "in other words", "that is", "i.e.", "e.g.",
]
STRIP_PREFIX = 'â€œâ€"\'ï¼ˆ()ã€[ã€ˆã€Šã€ã€Œ'  # å¥é¦–å¯å‰¥ç¦»çš„å¼•å·/æ‹¬å·


def starts_with_connective(sentence: str) -> bool:
    s = sentence.strip()
    s = s.lstrip(STRIP_PREFIX).strip().lower()
    for p in CN_CONNECTIVES:
        if s.startswith(p.lower()):
            return True
    for p in EN_CONNECTIVES:
        if s.startswith(p.lower()):
            return True
    return False


CN_STOPWORDS: Set[str] = {
    "çš„", "äº†", "å’Œ", "åŠ", "ä¸", "æˆ–", "è€Œ", "å¹¶", "åœ¨", "å¯¹", "æŠŠ", "è¢«", "æ˜¯", "å°±", "ä¹Ÿ", "éƒ½", "å¾ˆ", "æ›´", "è¿˜",
    "åˆ™", "å³", "ç­‰", "åŠå…¶", "ä»¥åŠ", "å…¶ä¸­", "é€šè¿‡", "å¯¹äº", "å…³äº", "ç”±äº", "å› æ­¤", "æ­¤å¤–", "å¦å¤–",
}
EN_STOPWORDS: Set[str] = {
    "the", "a", "an", "of", "to", "and", "or", "but", "for", "on", "in", "at",
    "is", "are", "was", "were", "be", "been", "being", "by", "with", "as",
    "that", "this", "these", "those", "it", "its", "from",
}


def extract_keywords(text: str) -> Set[str]:
    """
    æç®€å…³é”®è¯æå–ï¼š
    - è‹±æ–‡ï¼šæŒ‰å•è¯æå–ï¼Œå»åœç”¨è¯ï¼Œé•¿åº¦â‰¥2
    - ä¸­æ–‡ï¼šæå–è¿ç»­æ±‰å­—ä¸²ï¼Œä¿ç•™é•¿åº¦â‰¥2
    """
    tokens: Set[str] = set()

    # è‹±æ–‡å•è¯
    for w in re.findall(r"[A-Za-z]+", text):
        lw = w.lower()
        if len(lw) >= 2 and lw not in EN_STOPWORDS:
            tokens.add(lw)

    # è¿ç»­ä¸­æ–‡ä¸²
    for seq in re.findall(r"[\u4e00-\u9fff]+", text):
        if len(seq) >= 2:
            tokens.add(seq)

    # æ•°å­—ä¸²ï¼ˆå¦‚æ—¥æœŸ/é‡‘é¢ï¼‰
    for num in re.findall(r"\d{2,}", text):
        tokens.add(num)

    # ç®€å•å½’ä¸€åŒ–ï¼šå¸¸è§ç¬¦å·å»æ‰
    normalized = set()
    for t in tokens:
        t2 = re.sub(r"[ï¼Œã€‚,.\-_/\\:ï¼šï¼›;ã€\s]+", "", t)
        if t2:
            # è¿‡é•¿ä¸­æ–‡ä¸²æˆªå–ä¸ºå‰4-8é•¿åº¦ï¼Œå‡å°‘è¿‡æ‹Ÿåˆ
            if re.fullmatch(r"[\u4e00-\u9fff]+", t2) and len(t2) > 8:
                normalized.add(t2[:8])
            else:
                normalized.add(t2)
    return normalized


def jaccard_similarity(a: Set[str], b: Set[str]) -> float:
    if not a and not b:
        return 0.0
    return len(a & b) / max(1, len(a | b))


def semantic_chunking(
    text: str,
    target_chunk_size: int = 500,
    min_chunk_size: int = 350,
    max_chunk_size: int = 800,
    overlap_sentences: int = 1,
    similarity_threshold: float = 0.25,
    tail_compare_k: int = 3,
) -> List[str]:
    """
    è¯­ä¹‰åˆ‡ç‰‡ï¼š
    - å…ˆæŒ‰å¥å­åˆ‡åˆ†
    - æŒ‰ç›®æ ‡å¤§å°èšåˆå¥å­
    - ç»“åˆè¿æ¥è¯ä¸ä¸»é¢˜é‡å åº¦å†³å®šæ˜¯å¦ä¸ä¸‹ä¸€å¥åˆå¹¶
    - è¾¾åˆ°ä¸Šé™å¼ºåˆ¶æˆªæ–­
    - å¥å­çº§é‡å 
    """
    sentences = split_into_sentences(text)
    if not sentences:
        return []

    chunks: List[str] = []
    current_sentences: List[str] = []

    i = 0
    n = len(sentences)

    while i < n:
        next_sentence = sentences[i].strip()
        if not next_sentence:
            i += 1
            continue

        current_text = " ".join(current_sentences).strip()
        next_len = len(next_sentence)
        current_len = len(current_text)

        # å°½é‡å¡«å……åˆ° target_chunk_size
        if current_len + next_len <= target_chunk_size:
            current_sentences.append(next_sentence)
            i += 1
            continue

        # æœªè¾¾æœ€å°é˜ˆå€¼æ—¶ï¼Œå…è®¸ç»§ç»­è¿½åŠ 
        if current_len < min_chunk_size:
            current_sentences.append(next_sentence)
            i += 1
            # å¦‚æœè¶…å‡º maxï¼Œåˆ™é©¬ä¸Šæˆªæ–­
            if len(" ".join(current_sentences)) >= max_chunk_size:
                chunk_text = " ".join(current_sentences).strip()
                if chunk_text:
                    chunks.append(chunk_text)
                # æ„é€ é‡å 
                overlap_tail = current_sentences[-overlap_sentences:] if overlap_sentences > 0 else []
                current_sentences = overlap_tail[:]
            continue

        # è¯­ä¹‰åˆ¤æ–­ï¼šè¿æ¥è¯æˆ–ä¸»é¢˜é‡å  â†’ å€¾å‘ç»§ç»­åˆå¹¶
        should_merge = False
        if starts_with_connective(next_sentence):
            should_merge = True
        else:
            tail_scope = " ".join(current_sentences[-tail_compare_k:])
            sim = jaccard_similarity(extract_keywords(tail_scope), extract_keywords(next_sentence))
            if sim >= similarity_threshold:
                should_merge = True

        if should_merge and (current_len + next_len) <= max_chunk_size:
            current_sentences.append(next_sentence)
            i += 1
            continue

        # æˆªæ–­å¹¶å¤„ç†é‡å 
        chunk_text = " ".join(current_sentences).strip()
        if chunk_text:
            chunks.append(chunk_text)

        overlap_tail = current_sentences[-overlap_sentences:] if overlap_sentences > 0 else []
        current_sentences = overlap_tail[:]

        # å¦‚æœå½“å‰å¥è¿‡é•¿å¯¼è‡´æ— æ³•åˆå¹¶ï¼Œå•ç‹¬æˆå—ï¼ˆé˜²æ­¢æ­»å¾ªç¯/æé•¿å¥ï¼‰
        if not current_sentences and next_len >= max_chunk_size:
            chunks.append(next_sentence)
            i += 1

    # æ”¶å°¾
    tail_text = " ".join(current_sentences).strip()
    if tail_text:
        chunks.append(tail_text)

    return chunks


def print_chunk_analysis(chunks: List[str], method_name: str) -> None:
    """æ‰“å°åˆ‡ç‰‡åˆ†æç»“æœ"""
    print(f"\n{'='*60}")
    print(f"ğŸ“‹ {method_name}")
    print(f"{'='*60}")

    if not chunks:
        print("âŒ æœªç”Ÿæˆä»»ä½•åˆ‡ç‰‡")
        return

    total_length = sum(len(chunk) for chunk in chunks)
    avg_length = total_length / len(chunks)
    min_length = min(len(chunk) for chunk in chunks)
    max_length = max(len(chunk) for chunk in chunks)

    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   - åˆ‡ç‰‡æ•°é‡: {len(chunks)}")
    print(f"   - å¹³å‡é•¿åº¦: {avg_length:.1f} å­—ç¬¦")
    print(f"   - æœ€çŸ­é•¿åº¦: {min_length} å­—ç¬¦")
    print(f"   - æœ€é•¿é•¿åº¦: {max_length} å­—ç¬¦")
    print(f"   - é•¿åº¦æ–¹å·®: {max_length - min_length} å­—ç¬¦")

    print(f"\nğŸ“ åˆ‡ç‰‡å†…å®¹:")
    for i, chunk in enumerate(chunks, 1):
        print(f"   å— {i} ({len(chunk)} å­—ç¬¦):")
        print(f"   {chunk}")
        print()


if __name__ == "__main__":
    print("è¯­ä¹‰åˆ‡ç‰‡ç­–ç•¥æµ‹è¯•")
    text = """
ITä¹‹å®¶ 7 æœˆ 31 æ—¥æ¶ˆæ¯ï¼Œé˜¶è·ƒæ˜Ÿè¾°å®£å¸ƒæ–°ä¸€ä»£åŸºç¡€å¤§æ¨¡å‹ Step 3 æ­£å¼å¼€æºï¼ŒStep 3 API å·²ä¸Šçº¿é˜¶è·ƒæ˜Ÿè¾°å¼€æ”¾å¹³å°ï¼ˆplatform.stepfun.comï¼‰ï¼Œç”¨æˆ·ä¹Ÿå¯ä»¥åœ¨â€œé˜¶è·ƒ AIâ€å®˜ç½‘ï¼ˆstepfun.comï¼‰å’Œâ€œé˜¶è·ƒ AIâ€App è¿›è¡Œä½“éªŒã€‚

æ®ä»‹ç»ï¼ŒStep 3 çš„å¤šæ¨¡æ€èƒ½åŠ›å›´ç»•â€œè½»é‡è§†è§‰è·¯å¾„â€ä¸â€œç¨³å®šååŒè®­ç»ƒâ€å±•å¼€ï¼Œé‡ç‚¹è§£å†³è§†è§‰å¼•å…¥å¸¦æ¥çš„ token è´Ÿæ‹…ä¸è®­ç»ƒå¹²æ‰°é—®é¢˜ã€‚ä¸ºæ­¤ï¼Œå…¶é‡‡ç”¨ 5B Vision Encoderï¼Œå¹¶é€šè¿‡åŒå±‚ 2D å·ç§¯å¯¹è§†è§‰ç‰¹å¾è¿›è¡Œé™é‡‡æ ·ï¼Œå°†è§†è§‰ token æ•°é‡å‡å°‘åˆ°åŸæ¥çš„ 1/16ï¼Œå‡è½»ä¸Šä¸‹æ–‡é•¿åº¦å‹åŠ›ï¼Œæå‡æ¨ç†æ•ˆç‡ã€‚

ITä¹‹å®¶é™„å®˜æ–¹å¯¹ Step 3 æ¨¡å‹çš„ä»‹ç»å¦‚ä¸‹ï¼š

æ ¸å¿ƒè¦ç‚¹
Step 3 å…¼é¡¾æ™ºèƒ½ä¸æ•ˆç‡ï¼Œä¸“ä¸ºè¿½æ±‚æ€§èƒ½ä¸æˆæœ¬æè‡´å‡è¡¡çš„ä¼ä¸šå’Œå¼€å‘è€…è®¾è®¡ï¼Œæ—¨åœ¨é¢å‘æ¨ç†æ—¶ä»£æ‰“é€ æœ€é€‚åˆåº”ç”¨çš„æ¨¡å‹ã€‚

Step 3 é‡‡ç”¨ MoE æ¶æ„ï¼Œæ€»å‚æ•°é‡ 321Bï¼Œæ¿€æ´»å‚æ•°é‡ 38Bã€‚

Step 3 æ‹¥æœ‰å¼ºå¤§çš„è§†è§‰æ„ŸçŸ¥å’Œå¤æ‚æ¨ç†èƒ½åŠ›ï¼Œå¯å‡†ç¡®å®Œæˆè·¨é¢†åŸŸçš„å¤æ‚çŸ¥è¯†ç†è§£ã€æ•°å­¦ä¸è§†è§‰ä¿¡æ¯çš„äº¤å‰åˆ†æï¼Œä»¥åŠæ—¥å¸¸ç”Ÿæ´»ä¸­çš„å„ç±»è§†è§‰åˆ†æé—®é¢˜ã€‚

é€šè¿‡ MFAï¼ˆMulti-matrix Factorization Attentionï¼‰ & AFDï¼ˆAttention-FFN Disaggregationï¼‰çš„ä¼˜åŒ–ï¼Œåœ¨å„ç±»èŠ¯ç‰‡ä¸Šæ¨ç†æ•ˆç‡å‡å¤§å¹…æå‡ã€‚

é¢å‘ AFD åœºæ™¯çš„ StepMesh é€šä¿¡åº“å·²éšæ¨¡å‹ä¸€åŒå¼€æºï¼Œæä¾›å¯è·¨ç¡¬ä»¶çš„æ ‡å‡†éƒ¨ç½²æ¥å£ï¼Œæ”¯æŒå…³é”®æ€§èƒ½åœ¨å®é™…æœåŠ¡ä¸­çš„ç¨³å®šå¤ç°ã€‚

æ¨¡å‹é™æ—¶æŠ˜æ‰£ä¸­ï¼Œæ‰€æœ‰è¯·æ±‚å‡æŒ‰æœ€ä½ä»·æ ¼è®¡ç®—ï¼Œæ¯ç™¾ä¸‡ token ä»·æ ¼ä½è‡³è¾“å…¥ 1.5 å…ƒï¼Œè¾“å‡º 4 å…ƒã€‚

Step 3 API å·²ä¸Šçº¿é˜¶è·ƒæ˜Ÿè¾°å¼€æ”¾å¹³å°ï¼ˆplatform.stepfun.comï¼‰ï¼Œå¤§å®¶ä¹Ÿå¯ä»¥åœ¨â€œé˜¶è·ƒ AIâ€å®˜ç½‘ï¼ˆstepfun.comï¼‰å’Œâ€œé˜¶è·ƒ AIâ€Appï¼ˆåº”ç”¨å•†åº—æœç´¢ä¸‹è½½ï¼‰è¿›è¡Œä½“éªŒã€‚
""".strip()

    print(f"æµ‹è¯•æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")

    chunks = semantic_chunking(
        text,
        target_chunk_size=500,
        min_chunk_size=350,
        max_chunk_size=800,
        overlap_sentences=1,
        similarity_threshold=0.25,
        tail_compare_k=3,
    )
    print_chunk_analysis(chunks, "è¯­ä¹‰åˆ‡ç‰‡ï¼ˆåŸºäºå¥å­è¾¹ç•Œ + ä¸»é¢˜é‡å  + è¿æ¥è¯ï¼‰")
